# Mapping categorical values to numbers
mappings:
  a: 1
  b: 2
  c: 3
  d: 4
  "0": 0
  "None": 0

categorical_features:
  - "StoreType"
  - "Assortment"
  - "StateHoliday"

# write 'Random Forest/XGBoost in place of Linear Regression to use it or vice versa'
models:
  type: "XGBoost"
  params_RF:
    n_estimators: 20
    max_depth: 8
    random_state: 42
    n_jobs: -1
  params_XGB:
    n_estimators: 500
    learning_rate: 0.05
    max_depth: 6
    sub_sample: 0.8
    colsample_bytree: 0.8

# training data parameters
training_data:
  target: "Sales"
  features:
    - "Store"
    - "DayOfWeek"
    - "Promo"
    - "StateHoliday"
    - "SchoolHoliday"
    - "StoreType"
    - "Assortment"
    - "CompetitionDistance"
    - "Year"
    - "Month"
    - "Day"

data:
  train_ratio: 0.7
  cv_ratio: 0.15
  test_ratio: 0.15

# every path required in any code goes here
paths:
  train_dataset: 'data/train.csv'
  store_dataset: 'data/store.csv'
  model_1: 'models/RandomForest.pkl'
  model_2: 'models/LinearRegression.pkl'
  model_3: 'models/XGBoost.pkl'

# Hyperparameter tuning
tuning:

  random_search:
    n_iter: 40
    cv_splits: 5
    scoring: rmspe
    random_state: 42
    n_jobs: -1

  param_grid_RF:
    n_estimators: [50, 100, 200, 300, 500]
    max_depth: [null, 5, 8, 10, 15, 20]
    min_samples_split: [2, 5, 10, 20]
    min_samples_leaf: [1, 2, 4, 8]
    max_features: ["sqrt", "log2", null]

  param_grid_XGB:
    n_estimators: [300, 500, 700, 1000]
    learning_rate: [0.01, 0.03, 0.05, 0.1]
    max_depth: [4, 6, 8, 10]
    subsample: [0.6, 0.8, 1.0]
    colsample_bytree: [0.6, 0.8, 1.0]
    gamma: [0, 0.1, 0.3, 0.5]
    reg_alpha: [0, 0.1, 1]
    reg_lambda: [1, 1.5, 2]

PERFORMANCE_THRESHOLD_RMSPE: 0.20 # or 20%